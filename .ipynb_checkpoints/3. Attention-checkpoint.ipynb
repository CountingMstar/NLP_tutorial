{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ['ich mochte ein bier P', 'S i want a beer', 'i want a beer E']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list = \" \".join(sentences).split()\n",
    "word_list = list(set(word_list))\n",
    "word_dictionary = {w: i for i, w in enumerate(word_list)}\n",
    "number_dictionary = {i: w for i, w in enumerate(word_list)}\n",
    "dictionary_size = len(word_dictionary)  # vocab list\n",
    "dictionary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot = np.eye(dictionary_size)\n",
    "one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "          [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]]]),\n",
       " tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "          [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]]]),\n",
       " tensor([[4, 7, 9, 6, 8]]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sentence_to_batch(sentences):\n",
    "    input_ = [word_dictionary[word] for word in sentences[0].split()]\n",
    "    output_ = [word_dictionary[word] for word in sentences[1].split()]\n",
    "    target_ = [word_dictionary[word] for word in sentences[2].split()]\n",
    "    \n",
    "    onehot_input = [one_hot[i] for i in input_]\n",
    "    onehot_output = [one_hot[i] for i in output_]\n",
    "\n",
    "    input_tensor = torch.FloatTensor([onehot_input])\n",
    "    output_tensor = torch.FloatTensor([onehot_output])\n",
    "    target_tensor = torch.LongTensor([target_])\n",
    "    \n",
    "    return input_tensor, output_tensor, target_tensor\n",
    "\n",
    "\n",
    "sentence_to_batch(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Attention, self).__init__()\n",
    "        self.encoder_cell = nn.RNN(input_size=dictionary_size, hidden_size=hidden_size, dropout=0.2)\n",
    "        self.decoder_cell = nn.RNN(input_size=dictionary_size, hidden_size=hidden_size, dropout=0.2)\n",
    "        \n",
    "        self.out = nn.Linear(hidden_size * 2, dictionary_size)\n",
    "\n",
    "\n",
    "    def forward(self, encoder_inputs, hidden, decoder_inputs):\n",
    "        encoder_inputs = encoder_inputs.transpose(0, 1) \n",
    "        decoder_inputs = decoder_inputs.transpose(0, 1)\n",
    "\n",
    "        encoder_outputs, encoder_hidden = self.encoder_cell(encoder_inputs, hidden)\n",
    "\n",
    "        trained_attention = []\n",
    "        hidden = encoder_hidden\n",
    "        number_of_cells = len(decoder_inputs)\n",
    "        model = torch.empty([number_of_cells, 1, dictionary_size])\n",
    "\n",
    "        for i in range(number_of_cells):  \n",
    "            \n",
    "            decoder_output, hidden = self.decoder_cell(decoder_inputs[i].unsqueeze(0), hidden)\n",
    "            attention_weights = self.attention_weight(decoder_output, encoder_outputs) \n",
    "            trained_attn.append(attention_weights.squeeze().data.numpy())\n",
    "        \n",
    "            context = attention_weights.bmm(encoder_outputs.transpose(0, 1))\n",
    "            decoder_output = decoder_output.squeeze(0) \n",
    "            context = context.squeeze(1) \n",
    "     \n",
    "            model[i] = self.out(torch.cat((decoder_output, context), dim=1))\n",
    "\n",
    "        model = model.transpose(0, 1).squeeze(0)\n",
    "        \n",
    "        return model, trained_attention\n",
    "\n",
    "    def attention_weight(self, decoder_output, encoder_outputs): \n",
    "        number_of_cells = len(encoder_outputs)\n",
    "        attn_scores = torch.zeros(number_of_cells)\n",
    "\n",
    "        for i in range(number_of_cells):\n",
    "            attn_scores[i] = self.attention_score(decoder_output, encoder_outputs[i])\n",
    "        \n",
    "        attention_value = F.softmax(attn_scores).view(1, 1, -1)\n",
    "        return attention_value\n",
    "\n",
    "    def attention_score(self, decoder_output, encoder_output):\n",
    "        decoder_hidden_state = decoder_output\n",
    "        encoder_hidden_state = encoder_output\n",
    "        score = torch.dot(decoder_hidden_state.view(-1), encoder_hidden_state.view(-1))\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Attention(\n",
       "  (encoder_cell): RNN(11, 128, dropout=0.2)\n",
       "  (decoder_cell): RNN(11, 128, dropout=0.2)\n",
       "  (out): Linear(in_features=256, out_features=11, bias=True)\n",
       "  (out2): Linear(in_features=11, out_features=11, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Attention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 128 # number of hidden units in one cell\n",
    "\n",
    "hidden = torch.zeros(1, 1, hidden_size)\n",
    "\n",
    "model = Attention()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "input_batch, output_batch, target_batch = sentence_to_batch(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3., 4., 5., 6.])\n",
      "torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "a = torch.Tensor([1, 2, 3])\n",
    "b = torch.Tensor([4, 5, 6])\n",
    "\n",
    "c = torch.cat([a, b], dim=-1)\n",
    "print(c)\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/moonstar/anaconda3/envs/pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:48: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/moonstar/anaconda3/envs/pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [1, 11]], which is output 0 of SelectBackward, is at version 10; expected version 9 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_236516/2507862101.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'%04d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cost ='\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'{:.6f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [1, 11]], which is output 0 of SelectBackward, is at version 10; expected version 9 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
     ]
    }
   ],
   "source": [
    "# Train\n",
    "for epoch in range(2000):\n",
    "    optimizer.zero_grad()\n",
    "    output, _ = model(input_batch, hidden, output_batch)\n",
    "\n",
    "    loss = criterion(output, target_batch.squeeze(0))\n",
    "    if (epoch + 1) % 400 == 0:\n",
    "        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/moonstar/anaconda3/envs/pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:48: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.nn.functional' has no attribute 'Tanh'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_236516/3872668502.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtest_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictionary_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_dictionary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m'SPPPP'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mpredict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrained_attn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'->'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnumber_dictionary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_236516/418593620.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, encoder_inputs, hidden, decoder_inputs)\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m#             print('*************')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch.nn.functional' has no attribute 'Tanh'"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "test_batch = [np.eye(dictionary_size)[[word_dictionary[n] for n in 'SPPPP']]]\n",
    "test_batch = torch.FloatTensor(test_batch)\n",
    "predict, trained_attn = model(input_batch, hidden, test_batch)\n",
    "predict = predict.data.max(1, keepdim=True)[1]\n",
    "print(sentences[0], '->', [number_dictionary[n.item()] for n in predict.squeeze()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/moonstar/anaconda3/envs/pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  \"\"\"\n",
      "/home/moonstar/anaconda3/envs/pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:6: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUcAAAE2CAYAAADyN1APAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAARjElEQVR4nO3dfbBcdX3H8fcHQsIA0hHEQUFAQUWqongBH1DSgTZYx5mOMjoqqDga8KGiIjqtRe2oQ/FhwIqiqYzRGeyo6EjFZyoZdMqzbdGCBR8QAoJBngkkAb79Y0/ssv4Scm/u3rPcvF8zO8k9e3bP99y9951zzt4kqSokSQ+1Vd8DSNIkMo6S1GAcJanBOEpSg3GUpAbjKEkNxnFIkuVJzt2E9fZKUkmm5mKuPnT7d2Tfc2yuR9J+JFmR5PSZ3q/ZtaDvASbM8UD6HuKRIMlewG+AA6vqsp7H2ZjHAbf1PcQseRmwru8hxiHJcuB13Yf3A9cD3wA+UFX39DGTcRxSVXf0PYNmV1Xd1PcMs6Wqbt3c50iyTVVNamDPA44GtgFeCHwe2B54cx/DeFo9ZPi0OgMnJLkmyZokK5OcPPKQPZP8MMnqJFcm+csxzbUiyRlJPpHk1iSrkhyfZFGSTye5Pcl1SY4eeswzkpyX5N7uMcuT/NnI874uyc+6/bs5yRdHNr1Tkq8luSfJr5McNXTfb7pfL+1OXVcMPe8x3efjviRXJ3lnkrF8rXWv03uS/Krb158Nzzl8Wj10OeTlc/G6zdCCJJ9Mclt3+9j6z93oaXWShUlO6b42Vye5NMmSofsXd/v710kuSbIWWNLY5qRYU1U3VdX1VfVl4Czgb3qbpqq8dTdgOXBu9/uTgduBNwD7AM8D3tLdtxdQwC+AlwJPBr4I/AHYYQxzrQDuBD7YbeuEbvvfZXApYB/gQ8AaBqeR2wM3At8EngEcClwNfH3oOY8F7gPeBTwVeA5w4tD9BawEjuqe/2RgLbBHd/+B3TpLgF2BnbrlbwJ+BxwJPLH7/NwEvG1Mr9lHgP8Fjui292rgHuAlQ/txZB+v2wxf57uATwH7Aq8A7gDeNXT/6UPrnwVcBLwIeBLwtu412r+7f3G3vz8D/qpbZ5e+9/PhvveGlv0zcEtvM/X9SZmk2/oXCNihC8dxG1hv/TfZsUPLduuWHTKGuVYAFw59HGAV8G9Dy7bpvjGO7AJ1B/CoofvXf6Ps0328EvinjWyzgJOHPl4ArAaOGvkcTI087jrg6JFl7wCuHMPnZXvgXuCFI8tPA74ztB+jcZyT122Gr/PVQIaW/QOwcuj+07vf7w08SPeH1dD63wQ+M/Kav7zvfduEfX9IHIGDgFuAr/Q1k9cc2/YDFgH//jDrXTH0+xu7Xx87lomGtlVVleT3DI4I1i9bl+S2bvv7AFdU1V1Dj/8PBt9M+yW5k0EUNnn/qur+JKvYyP4l2QV4AvC5JGcM3bWA8bzRtR+wLfC9JMP/gso2wLUbedxcvm7TdVF1dehcCHwoyY4j6x3A4HN6ZfKQT+0i4Ecj607yG2bDjkhyN4Ovl22Ac4C/7WsY47h5/nhhuwsWjO867uhF9NrAsofb/nT+GabpPv/6+45jEONxW7+9lzI4Yh22sTcd5vJ1G5etGLweB/Kn+3rvyMe9vNs7AxcASxnsz43V8xtHxrHtKgbX7w4Drul5lpm4CnhDkkcNHT0+n8E31FVV9fskNzDYvx/OcBtru1+3Xr+gqm5OciOwd1V9aYbPOx1XMnid9qyq0aOlR6qDk2To6PG5DEJx58gR4n8yOHLctarOn+shx2R1Vf2y7yHWM44NVXVXkk8CJydZw+BPtJ2B51TVGRt/9EQ4C/hH4EtJ3g88Gvgc8I2hL76PAKcmuRn4NrAdcFhVfWITt/F7BkcoS5JcC9xXgx+F+gDwqSS3A99hcHp0ALBbVY2+279Zutfp48DHMyjHBQyuFz8XeLCqls3m9ubI44HTknyGwZtpJwIfHl2pqq5OchawPMkJwE+BnRhcZ/x1VX1j7kaen4zjhv0dgx8ePgnYHbgZmIujoc1WVau7H+k4DbiEwZtL5zB4Z3v9Omd0P9pxAnAKcCuDmG3qNu5P8nbg/QyC+GNgcVV9Psk9DL6pT2YQ0P8BxvU3O05i8Nq8GziDwbv6/wV8dEzbG7ezGByNX8zgtPlM4NQNrHsM8D4G+7o7g9fwEmC+HEn2Kg+99itJgkfeRWhJmhPGUZIajKMkNRhHSWowjpLUYBwlqcE4TlOSpX3PMA7zdb9g/u6b+zVexnH6JuKFG4P5ul8wf/fN/Roj4yhJDfPib8gszKLalu3nZFvrWMM2LJqTbc2lud6vLFw4Z9ta++BqFm613dxs69HbzMl2AB5YfQ9bbzc3X/cAD87Rl8cDd9/N1jvsMDcbA9Zet/KWqtpldPm8+LvV27I9B291eN9jzL558AfXhizYfc++RxiL61+2W98jjM3dT3yg7xHG4rdvPfG3reWeVktSg3GUpAbjKEkNxlGSGoyjJDUYR0lqMI6S1GAcJanBOEpSg3GUpAbjKEkNxlGSGoyjJDUYR0lqMI6S1GAcJanBOEpSg3GUpAbjKEkNxlGSGiY6jkmWJzm37zkkbXkm/X8fPB5I30NI2vJMdByr6o6+Z5C0ZfK0WpIaJjqOktSXiT6t3pgkS4GlANuyXc/TSJpvHrFHjlW1rKqmqmpqGxb1PY6keeYRG0dJGifjKEkNxlGSGoyjJDVM9LvVVfX6vmeQtGXyyFGSGoyjJDUYR0lqMI6S1GAcJanBOEpSg3GUpAbjKEkNxlGSGoyjJDUYR0lqMI6S1GAcJanBOEpSg3GUpAbjKEkNxlGSGoyjJDUYR0lqSFX1PcNmm9p/27rk+0/oe4xZt2S3Z/c9wvjMg687zQ/n1dmXV9XU6HKPHCWpwThKUoNxlKQG4yhJDcZRkhqMoyQ1GEdJajCOktRgHCWpwThKUoNxlKQG4yhJDcZRkhqMoyQ1GEdJajCOktRgHCWpwThKUoNxlKQG4yhJDcZRkhqMoyQ1GEdJapjIOCZZkeT0vueQtOWayDhKUt8eNo5JjkhyV5IF3cf7JKkknx1a58NJzkuydZIzk/wmyb1JrknyniRbDa27PMm5SY5PckOS25J8Icl26+8HDgXe2m2nkuw12zsuSRuzYBPW+QmwLTAFXAQsBm7pfl1vMfA9BrG9AXgFsAo4CFgG/AE4c2j9FwK/Aw4HngB8FbgaOBk4HngK8Avg77v1V01vtyRp8zzskWNV3Q1cDvxFt2gxcDqwZ5LHdUd8BwIrqmpdVb2/qi6tqmur6qvAZ4FXjTztncBxVXVVVf0A+BpwWLe9O4C1wOqquqm7PTA6V5KlSS5LctmqP/zJ3ZK0WTb1muMK/v9I8VDgu8DF3bLnA/cDlwAkOa6L1qokdwPvBPYYeb4rR4J3I/DY6QxeVcuqaqqqpnbZeevpPFSSHtZ04viCJE8DdmRwJLmCwdHkYuDCqlqb5JXAacByYAnwLOAzwMKR51s38nFNYxZJGrtNueYIg+uOi4D3AD+pqgeSrAD+BbiZwfVGgEOAi6vqjz+Gk2TvGcy1FvBwUFJvNulobei641HA+d3ii4DdgecyOIqEwZsqByR5cZInJzmJwWn4dF0LHJRkrySPGX63W5LmwnSis4LBkeYKgKq6j8F1xzV01xuBzzF45/nLwKXAXsAnZjDXxxkcPV7J4J3q0WuWkjRWqaq+Z9hsU/tvW5d8/wl9jzHrluz27L5HGJ958HWn+eG8OvvyqpoaXe7pqiQ1GEdJajCOktRgHCWpwThKUoNxlKQG4yhJDcZRkhqMoyQ1GEdJajCOktRgHCWpwThKUoNxlKQG4yhJDcZRkhqMoyQ1GEdJajCOktRgHCWpYVP/3+qJdvUV27Hk8c/qe4wxmL//CdXWO+/U9whjcd0b9+17hLG5Z591fY8wHm88u7nYI0dJajCOktRgHCWpwThKUoNxlKQG4yhJDcZRkhqMoyQ1GEdJajCOktRgHCWpwThKUoNxlKQG4yhJDcZRkhqMoyQ1GEdJajCOktRgHCWpwThKUoNxlKQG4yhJDcZRkhqMoyQ1GEdJapioOCY5IsmPk9yW5NYk30/ytL7nkrTlmag4AtsDpwEHAYuBO4BvJVk4umKSpUkuS3LZOtbM6ZCS5r8FfQ8wrKq+PvxxkmOAOxnE8icj6y4DlgHsmJ1qrmaUtGWYqCPHJHsn+XKSXyW5E7iZwYx79DyapC3MRB05AucCK4FjgRuA+4ErgT85rZakcZqYOCbZGdgXeEtVnd8tO4AJmlHSlmOSwnMbcAvwpiTXA7sBH2Nw9ChJc2pirjlW1YPAK4FnAj8HPg2cBL4VLWnuTdKRI1X1I+DpI4t36GMWSVu2iTlylKRJYhwlqcE4SlKDcZSkBuMoSQ3GUZIajKMkNRhHSWowjpLUYBwlqcE4SlKDcZSkBuMoSQ3GUZIajKMkNRhHSWowjpLUYBwlqcE4SlJDqqrvGTbbs/ZfWD/4zmP6HmPWvfbPX9z3CGPzwJ139j2CBMB5dfblVTU1utwjR0lqMI6S1GAcJanBOEpSg3GUpAbjKEkNxlGSGoyjJDUYR0lqMI6S1GAcJanBOEpSg3GUpAbjKEkNxlGSGoyjJDUYR0lqMI6S1GAcJanBOEpSg3GUpAbjKEkN04pjkhVJTh/XMJI0KTxylKSGiY9jkoV9zyBpyzOTOC5I8skkt3W3jyXZCgYhS3JKkpVJVie5NMmS4Qcn2S/Jt5PcleT3Sf41ya5D9y9Pcm6S9yZZCazcvF2UpOmbSRxf0z3uecCxwFLgHd19XwAOBV4NPB34IvCtJPsDJHkccAHwc+Ag4HBgB+Cc9YHtHAo8EzgCOGwGM0rSZlkwg8f8Dnh7VRXwiyRPAd6V5BzgVcBeVXVdt+7pSQ5nENG3AG8G/ruq3rv+yZK8FrgVmAIu6RbfB7yhqtZsaIgkSxmEmd1323oGuyFJGzaTI8eLujCudyGwG3AIEODKJHevvwEvAfbu1n0O8KKR+6/v7tt76Dl/vrEwAlTVsqqaqqqpnXee+Eunkh5hZnLkuDEFHAisG1l+b/frVsC3gXc3Hnvz0O/vmeW5JGlaZhLHg5Nk6OjxucCNDI4gA+xaVedv4LE/BV4B/LaqRgMqSRNjJuejjwdOS/LUJEcCJwKnVtXVwFnA8iRHJnlSkqkk707ysu6xnwb+DPhKkoO7dQ5PsizJo2ZljyRpFszkyPEsYGvgYgan0WcCp3b3HQO8D/gosDuDN1ouAc4HqKobk7wAOBn4HrAtcB3wA2Cj1xglaS5NK45VtXjow7c17l8HfLC7beg5rgGO3Mj9r5/OTJI0Dr7NK0kNxlGSGoyjJDUYR0lqMI6S1GAcJanBOEpSg3GUpAbjKEkNxlGSGoyjJDUYR0lqMI6S1GAcJanBOEpSg3GUpAbjKEkNxlGSGoyjJDXM9v9b3YtfX7EDr9njkL7HmHVZcF/fI4zNVk/ft+8RxuJXr3503yOMzaLb0vcI4/HRs5uLPXKUpAbjKEkNxlGSGoyjJDUYR0lqMI6S1GAcJanBOEpSg3GUpAbjKEkNxlGSGoyjJDUYR0lqMI6S1GAcJanBOEpSg3GUpAbjKEkNxlGSGoyjJDUYR0lqMI6S1GAcJalhYuKYZHmSatwu6ns2SVueBX0PMOI84OiRZWv7GETSlm3S4rimqm7qewhJmpjTakmaJJMWxyOS3D1yO6W1YpKlSS5Lctk61sz1nJLmuUk7rb4AWDqy7PbWilW1DFgGsGN2qvGOJWlLM2lxXF1Vv+x7CEmatNNqSZoIk3bkuCjJriPLHqiqVb1MI2mLNWlxPBz43ciyG4Dde5hF0hZsYk6rq+r1VZXGzTBKmnMTE0dJmiTGUZIajKMkNRhHSWowjpLUYBwlqcE4SlKDcZSkBuMoSQ3GUZIajKMkNRhHSWowjpLUYBwlqcE4SlKDcZSkBuMoSQ3GUZIajKMkNRhHSWpIVfU9w2ZLsgr47Rxt7jHALXO0rbk0X/cL5u++uV+zY8+q2mV04byI41xKcllVTfU9x2ybr/sF83ff3K/x8rRakhqMoyQ1GMfpW9b3AGMyX/cL5u++uV9j5DVHSWrwyFGSGoyjJDUYR0lqMI6S1GAcJanh/wDPpKPvQ2hTtwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show Attention\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.matshow(trained_attn, cmap='viridis')\n",
    "ax.set_xticklabels([''] + sentences[0].split(), fontdict={'fontsize': 14})\n",
    "ax.set_yticklabels([''] + sentences[2].split(), fontdict={'fontsize': 14})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
