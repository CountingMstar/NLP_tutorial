{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/moonstar/anaconda3/lib/python3.7/site-packages (3.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/moonstar/anaconda3/lib/python3.7/site-packages (from nltk) (2022.7.25)\n",
      "Requirement already satisfied: click in /home/moonstar/anaconda3/lib/python3.7/site-packages (from nltk) (7.0)\n",
      "Requirement already satisfied: tqdm in /home/moonstar/anaconda3/lib/python3.7/site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: joblib in /home/moonstar/anaconda3/lib/python3.7/site-packages (from nltk) (0.14.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"I am a elementary school student\", \"And I am a boy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['I', 'am', 'a', 'elementary', 'school', 'student'], ['And', 'I', 'am', 'a', 'boy']]\n"
     ]
    }
   ],
   "source": [
    "# Word tokenized sentence\n",
    "from nltk import word_tokenize\n",
    "\n",
    "text_tokenzied = [word_tokenize(sentence) for sentence in text]\n",
    "print(text_tokenzied)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'I': 2, 'am': 2, 'a': 2, 'elementary': 1, 'school': 1, 'student': 1, 'And': 1, 'boy': 1})\n"
     ]
    }
   ],
   "source": [
    "# Count the words\n",
    "from collections import Counter\n",
    "\n",
    "vocab_counter = Counter()\n",
    "for sentence in text_tokenzied:\n",
    "    vocab_counter.update(sentence)\n",
    "    \n",
    "print(vocab_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'a', 'elementary', 'school', 'student', 'And', 'boy']\n"
     ]
    }
   ],
   "source": [
    "# Vocabulary set\n",
    "vocab = []\n",
    "for key, value in vocab_counter.items():\n",
    "    vocab.append(key)\n",
    "\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 2, 2, 1, 1, 1], [1, 2, 2, 2, 1]]\n"
     ]
    }
   ],
   "source": [
    "# Count vector\n",
    "text_count_vector = []\n",
    "\n",
    "for sentence in text_tokenzied:\n",
    "    sentence_vector = []\n",
    "    \n",
    "    for word in sentence:\n",
    "        sentence_vector.append(vocab_counter[word])\n",
    "    text_count_vector.append(sentence_vector)\n",
    "    \n",
    "print(text_count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 1 1 1 1 1]]\n",
      "vocabulary : {'am': 0, 'elementary': 3, 'school': 4, 'student': 5, 'and': 1, 'boy': 2}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vector = CountVectorizer()\n",
    "text = [\"I am a elementary school student. And I am a boy\"]\n",
    "\n",
    "vocab_counter_sklearn = vector.fit_transform(text).toarray()\n",
    "\n",
    "# Count of vocabulary\n",
    "print(vocab_counter_sklearn)\n",
    "\n",
    "# Index of list\n",
    "print('vocabulary :',vector.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"I am a great great elementary school student\", \"And I am a boy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['I', 'am', 'a', 'great', 'great', 'elementary', 'school', 'student'], ['And', 'I', 'am', 'a', 'boy']]\n"
     ]
    }
   ],
   "source": [
    "# Word tokenized sentence\n",
    "from nltk import word_tokenize\n",
    "\n",
    "text_tokenzied = [word_tokenize(sentence) for sentence in text]\n",
    "print(text_tokenzied)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['am', 'great', 'great', 'elementary', 'school', 'student'], ['And', 'am', 'boy']]\n"
     ]
    }
   ],
   "source": [
    "# Remove stopwords that are too short.\n",
    "text_tokenzied2 = []\n",
    "for sentence in text_tokenzied:\n",
    "    sent = []\n",
    "    for word in sentence:\n",
    "        if len(word) >= 2:\n",
    "            sent.append(word)\n",
    "    text_tokenzied2.append(sent)\n",
    "\n",
    "print(text_tokenzied2)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['am', 'great', 'elementary', 'school', 'student', 'And', 'boy']\n"
     ]
    }
   ],
   "source": [
    "# Vocabulary list\n",
    "from collections import Counter\n",
    "\n",
    "vocab_counter = Counter()\n",
    "for sentence in text_tokenzied2:\n",
    "    vocab_counter.update(sentence)\n",
    "\n",
    "vocab = []\n",
    "for key, value in vocab_counter.items():\n",
    "    vocab.append(key)\n",
    "    \n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Counter({'great': 2, 'am': 1, 'elementary': 1, 'school': 1, 'student': 1}), Counter({'And': 1, 'am': 1, 'boy': 1})]\n"
     ]
    }
   ],
   "source": [
    "# Count word of each sentence\n",
    "from collections import Counter\n",
    "\n",
    "count = []\n",
    "for sentence in text_tokenzied2:\n",
    "    vocab_counter = Counter()\n",
    "    vocab_counter.update(sentence)\n",
    "    count.append(vocab_counter)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['am', 'great', 'elementary', 'school', 'student', 'And', 'boy']\n",
      "[1, 2, 1, 1, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "def TF(vocab, counter):\n",
    "    vector = []\n",
    "    for word in vocab:\n",
    "        if counter[word] != False:\n",
    "            vector.append(counter[word])\n",
    "        else:\n",
    "            vector.append(0)\n",
    "    return vector\n",
    "\n",
    "print(vocab)\n",
    "print(TF(vocab, count[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['am', 'great', 'elementary', 'school', 'student', 'And', 'boy']\n",
      "[2, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "def DF(text_tokenzied2, vocab):\n",
    "    text = []\n",
    "    for sentence in text_tokenzied2:\n",
    "        for word in list(set(sentence)):\n",
    "            text.append(word)\n",
    "    vocab_counter = Counter()\n",
    "    vocab_counter.update(text)\n",
    "    \n",
    "    df = []\n",
    "    for word in vocab:\n",
    "        df.append(vocab_counter[word])\n",
    "    return df\n",
    "\n",
    "print(vocab)\n",
    "print(DF(text_tokenzied2, vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['am', 'great', 'elementary', 'school', 'student', 'And', 'boy']\n",
      "[0.5945348918918356, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def IDF(df, n):\n",
    "    idf = []\n",
    "    for i in df:\n",
    "        idf.append(math.log((n)/(i+1))+1)\n",
    "    return idf\n",
    "\n",
    "print(vocab)\n",
    "print(IDF(DF(text_tokenzied2, vocab), len(text_tokenzied2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['am', 'great', 'elementary', 'school', 'student', 'And', 'boy']\n",
      "[0.5945348918918356, 2.0, 1.0, 1.0, 1.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "def TFIDF(tf, idf):\n",
    "    product = [x*y for x, y in zip(tf, idf)]\n",
    "    return product\n",
    "\n",
    "print(vocab)\n",
    "print(TFIDF(TF(vocab, count[0]), IDF(DF(text_tokenzied2, vocab), len(text_tokenzied2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['am', 'great', 'elementary', 'school', 'student', 'And', 'boy']\n",
      "[[0.5945348918918356, 2.0, 1.0, 1.0, 1.0, 0.0, 0.0], [0.5945348918918356, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0]]\n"
     ]
    }
   ],
   "source": [
    "tfidf = []\n",
    "for c in count:\n",
    "    tfidf.append(TFIDF(TF(vocab, c), IDF(DF(text_tokenzied2, vocab), len(text_tokenzied2))))\n",
    "\n",
    "print(vocab)\n",
    "print(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.25969799 0.         0.         0.36499647 0.72999294 0.36499647\n",
      "  0.36499647]\n",
      " [0.44943642 0.6316672  0.6316672  0.         0.         0.\n",
      "  0.        ]]\n",
      "{'am': 0, 'great': 4, 'elementary': 3, 'school': 5, 'student': 6, 'and': 1, 'boy': 2}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidfv = TfidfVectorizer().fit(text)\n",
    "print(tfidfv.transform(text).toarray())\n",
    "print(tfidfv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
