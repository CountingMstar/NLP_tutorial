{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "# model_name = \"gpt2-xl\"\n",
    "# model_name = \"gpt2-large\"\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greedy search decoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input</th>\n",
       "      <th>Choice 1</th>\n",
       "      <th>Choice 2</th>\n",
       "      <th>Choice 3</th>\n",
       "      <th>Choice 4</th>\n",
       "      <th>Choice 5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Transformers are the</td>\n",
       "      <td>most (9.76%)</td>\n",
       "      <td>same (2.94%)</td>\n",
       "      <td>only (2.87%)</td>\n",
       "      <td>best (2.38%)</td>\n",
       "      <td>first (1.77%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Transformers are the most</td>\n",
       "      <td>common (22.90%)</td>\n",
       "      <td>powerful (6.88%)</td>\n",
       "      <td>important (6.32%)</td>\n",
       "      <td>popular (3.95%)</td>\n",
       "      <td>commonly (2.14%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Transformers are the most common</td>\n",
       "      <td>type (15.06%)</td>\n",
       "      <td>types (3.31%)</td>\n",
       "      <td>form (1.91%)</td>\n",
       "      <td>way (1.89%)</td>\n",
       "      <td>and (1.49%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Transformers are the most common type</td>\n",
       "      <td>of (83.13%)</td>\n",
       "      <td>in (3.16%)</td>\n",
       "      <td>. (1.92%)</td>\n",
       "      <td>, (1.63%)</td>\n",
       "      <td>for (0.88%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Transformers are the most common type of</td>\n",
       "      <td>particle (1.55%)</td>\n",
       "      <td>object (1.02%)</td>\n",
       "      <td>light (0.71%)</td>\n",
       "      <td>energy (0.67%)</td>\n",
       "      <td>objects (0.66%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Transformers are the most common type of particle</td>\n",
       "      <td>. (14.26%)</td>\n",
       "      <td>in (11.57%)</td>\n",
       "      <td>that (10.19%)</td>\n",
       "      <td>, (9.57%)</td>\n",
       "      <td>accelerator (5.81%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Transformers are the most common type of parti...</td>\n",
       "      <td>They (17.48%)</td>\n",
       "      <td>\\n (15.19%)</td>\n",
       "      <td>The (7.06%)</td>\n",
       "      <td>These (3.09%)</td>\n",
       "      <td>In (3.07%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Transformers are the most common type of parti...</td>\n",
       "      <td>are (38.78%)</td>\n",
       "      <td>have (8.14%)</td>\n",
       "      <td>can (7.98%)</td>\n",
       "      <td>'re (5.04%)</td>\n",
       "      <td>consist (1.57%)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Input           Choice 1  \\\n",
       "0                               Transformers are the       most (9.76%)   \n",
       "1                          Transformers are the most    common (22.90%)   \n",
       "2                   Transformers are the most common      type (15.06%)   \n",
       "3              Transformers are the most common type        of (83.13%)   \n",
       "4           Transformers are the most common type of   particle (1.55%)   \n",
       "5  Transformers are the most common type of particle         . (14.26%)   \n",
       "6  Transformers are the most common type of parti...      They (17.48%)   \n",
       "7  Transformers are the most common type of parti...       are (38.78%)   \n",
       "\n",
       "            Choice 2            Choice 3          Choice 4  \\\n",
       "0       same (2.94%)        only (2.87%)      best (2.38%)   \n",
       "1   powerful (6.88%)   important (6.32%)   popular (3.95%)   \n",
       "2      types (3.31%)        form (1.91%)       way (1.89%)   \n",
       "3         in (3.16%)           . (1.92%)         , (1.63%)   \n",
       "4     object (1.02%)       light (0.71%)    energy (0.67%)   \n",
       "5        in (11.57%)       that (10.19%)         , (9.57%)   \n",
       "6        \\n (15.19%)         The (7.06%)     These (3.09%)   \n",
       "7       have (8.14%)         can (7.98%)       're (5.04%)   \n",
       "\n",
       "               Choice 5  \n",
       "0         first (1.77%)  \n",
       "1      commonly (2.14%)  \n",
       "2           and (1.49%)  \n",
       "3           for (0.88%)  \n",
       "4       objects (0.66%)  \n",
       "5   accelerator (5.81%)  \n",
       "6            In (3.07%)  \n",
       "7       consist (1.57%)  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "input_txt = \"Transformers are the\"\n",
    "\n",
    "# word to vector\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "\n",
    "iterations = []\n",
    "n_steps = 8\n",
    "choices_per_step = 5\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(n_steps):\n",
    "        \n",
    "        # dictionary\n",
    "        iteration = dict()\n",
    "        iteration[\"Input\"] = tokenizer.decode(input_ids[0])\n",
    "\n",
    "        output = model(input_ids=input_ids)\n",
    "        \n",
    "#         The very first embedding vectors are just the input tokens (in embedding space)\n",
    "#         The very last embedding vectors are just the output logits (in embedding space)\n",
    "        # select the last logit\n",
    "        next_token_logits = output.logits[0, -1, :]\n",
    "        \n",
    "        # softmax\n",
    "        next_token_probs = torch.softmax(next_token_logits, dim=-1)\n",
    "        \n",
    "        # torch.argsort: sort the list and return the index of a elements\n",
    "        sorted_ids = torch.argsort(next_token_probs, dim=-1, descending=True)\n",
    "        \n",
    "        for choice_idx in range(choices_per_step):\n",
    "            # token_id: candidate of the next word's embedding vector\n",
    "            token_id = sorted_ids[choice_idx]\n",
    "            token_prob = next_token_probs[token_id].cpu().numpy()\n",
    "\n",
    "            token_choice = (\n",
    "                f\"{tokenizer.decode(token_id)} ({100 * token_prob:.2f}%)\"\n",
    "            )\n",
    "            iteration[f\"Choice {choice_idx+1}\"] = token_choice\n",
    "        # pick the next word that is max probability\n",
    "        input_ids = torch.cat([input_ids, sorted_ids[None, 0, None]], dim=-1)\n",
    "        iterations.append(iteration)\n",
    "        \n",
    "pd.DataFrame(iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers are the most common type of particle. They are\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "output = model.generate(input_ids, max_new_tokens=n_steps, do_sample=False)\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\n",
      "\"The unicorns were very intelligent, and they were very intelligent,\" said Dr. David S. Siegel, a professor of anthropology at the University of California, Berkeley. \"They were very intelligent, and they were very intelligent, and they were very intelligent, and they were very intelligent, and they were very intelligent, and they were very intelligent, and they were very intelligent, and they were very\n"
     ]
    }
   ],
   "source": [
    "max_length = 128\n",
    "input_txt = \"\"\"In a shocking finding, scientist discovered \\\n",
    "a herd of unicorns living in a remote, previously unexplored \\\n",
    "valley, in the Andes Mountains. Even more surprising to the \\\n",
    "researchers was the fact that the unicorns spoke perfect English.\\n\\n\n",
    "\"\"\"\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "output_greedy = model.generate(input_ids, max_length=max_length, \n",
    "                               do_sample=False)\n",
    "print(tokenizer.decode(output_greedy[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beam search decoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def log_probs_from_logits(logits, labels):\n",
    "    logp = F.log_softmax(logits, dim=-1)\n",
    "    \n",
    "    # logp: input\n",
    "    # labels: index\n",
    "    logp_label = torch.gather(logp, 2, labels.unsqueeze(2)).squeeze(-1)\n",
    "    return logp_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_logprob(model, labels, input_len=0):\n",
    "    with torch.no_grad():\n",
    "        output = model(labels)\n",
    "\n",
    "        log_probs = log_probs_from_logits(\n",
    "            output.logits[:, :-1, :], labels[:, 1:])\n",
    "        \n",
    "        # sum of the log probabilities\n",
    "        seq_log_prob = torch.sum(log_probs[:, input_len:])\n",
    "    return seq_log_prob.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\n",
      "\"The unicorns were very intelligent, and they were very intelligent,\" said Dr. David S. Siegel, a professor of anthropology at the University of California, Berkeley. \"They were very intelligent, and they were very intelligent, and they were very intelligent, and they were very intelligent, and they were very intelligent, and they were very intelligent, and they were very intelligent, and they were very\n",
      "\n",
      "log prob: -83.33\n"
     ]
    }
   ],
   "source": [
    "# greedy search\n",
    "logp = sequence_logprob(model, output_greedy, input_len=len(input_ids[0]))\n",
    "print(tokenizer.decode(output_greedy[0]))\n",
    "print(f\"\\nlog prob: {logp:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128])\n",
      "tensor([[  818,   257, 14702,  4917,    11, 11444,  5071,   257, 27638,   286,\n",
      "         28000, 19942,  2877,   287,   257,  6569,    11,  4271, 31286,  1850,\n",
      "         19272,    11,   287,   262,   843,   274, 21124,    13,  3412,   517,\n",
      "          6452,   284,   262,  4837,   373,   262,  1109,   326,   262, 28000,\n",
      "         19942,  5158,  2818,  3594,    13,   628,   198,   464,  2050,    11,\n",
      "          3199,   287,   262,  3989, 30641,   286,   262,  2351,  8581,   286,\n",
      "         13473,    11,   318,   262,   717,   284,   905,   326, 28000, 19942,\n",
      "           389,  1498,   284, 10996,   351,   530,  1194,    13,   628,   198,\n",
      "             1,  1212,   318,   262,   717,  2050,   284,   905,   326, 28000,\n",
      "         19942,   389,  1498,   284, 10996,   351,   530,  1194,   553,   531,\n",
      "          2050,   763,    12,  9800,  1583,    13,  3899,   311, 28210,    11,\n",
      "           257,  6240,   286, 17219,   379,   262,  2059,   286,  3442,    11,\n",
      "          2986,  9500,    13,   366,  1212,   318,   262,   717]],\n",
      "       device='cuda:0')\n",
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\n",
      "The study, published in the journal Proceedings of the National Academy of Sciences, is the first to show that unicorns are able to communicate with one another.\n",
      "\n",
      "\n",
      "\"This is the first study to show that unicorns are able to communicate with one another,\" said study co-author Dr. Michael Siegel, a professor of biology at the University of California, San Diego. \"This is the first\n",
      "\n",
      "log prob: -75.20\n"
     ]
    }
   ],
   "source": [
    "# beam search\n",
    "output_beam = model.generate(input_ids, max_length=max_length, num_beams=15, \n",
    "                             do_sample=False)\n",
    "\n",
    "logp = sequence_logprob(model, output_beam, input_len=len(input_ids[0]))\n",
    "print(tokenizer.decode(output_beam[0]))\n",
    "print(f\"\\nlog prob: {logp:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
